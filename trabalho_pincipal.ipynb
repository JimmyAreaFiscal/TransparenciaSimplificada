{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5224c890",
   "metadata": {},
   "source": [
    "# Entrega Aplicando Conhecimento A2\n",
    "\n",
    "Neste documento, que compõe a entrega necessária para o exercício A2 - Aplicando Conhecimento, os seguintes itens serão desenvolvidos:\n",
    "\n",
    "1. Definição de linguagem de programação usada no Projeto\n",
    "2. Análise exploratória da base de dados escolhida\n",
    "3. Tratamento da base de dados \n",
    "4. Definição e descrição das bases teóricas dos métodos \n",
    "5. Definição e descrição de como será calculada a acurácia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271dace2",
   "metadata": {},
   "source": [
    "## Definição de linguagem de programação usada no Projeto\n",
    "\n",
    "Para o projeto, será utilizada a linguagem de programação Python, em virtude dos seguintes fatores:\n",
    "\n",
    "1. Versatilidade: permite-se escrever tanto o backend quanto um REST API e frontend por meio de uma mesma linguagem, facilitando o desenvolvimento do protótipo;\n",
    "\n",
    "2. Adequação à projetos de IA: python se tornou uma das linguagens naturalmente utilizadas para projetos envolvendo IA generativa, que é o caso deste trabalho;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5dd256",
   "metadata": {},
   "source": [
    "## Tratamento da base de dados\n",
    "\n",
    "Inicialmente, faz-se necessário ressaltar que a \"base de dados\" advém de um único documento com 179 páginas. Assim, para permitir uma análise exploratória ou qualquer outra atividade de análise, deve-se transformar o documento (dado não estruturado) em uma base de dados propriamente dita (no nosso caso, com a extração de pedaços - <i>chunks</i> - e transformação em vetores - por meio de modelos de <i>embeddings</i>).\n",
    "\n",
    "Para isso, será utilizada a biblioteca opensource <b><a href='https://docling-project.github.io/docling/'>Docling</a></b>, que permite o processamento de textos em pdf com extração de imagens por OCR, além da biblioteca de IA generativa <a href='https://www.langchain.com/'>LangChain</a>, que permite a utilização de modelos de embeddings e de LLMs no trabalho. \n",
    "\n",
    "(Para maiores informações, acessar o website das bibliotecas).\n",
    "\n",
    "A ideia da preparação de dados, neste caso, é permitir que o texto (dado não estruturado) seja transformado em um conjunto de vetores de valores (embeddings) para, posteriormente, ser usado por um sistema ou modelo para responder perguntas dos usuários."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca690e",
   "metadata": {},
   "source": [
    "#### Transformação do .pdf em um markdown\n",
    "\n",
    "Através dessa transformação, o documento poderá ser posteriormente quebrado em seções, facilitando a criação de <i>chunks</i> para a criação da vector store. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = \"BalancoGeralEstadoRS.pdf\"\n",
    "\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13655703",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.document.export_to_markdown()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f8f1c",
   "metadata": {},
   "source": [
    "#### Transformação do markdown em chunks \n",
    "\n",
    "Após a transformação em markdown, permite-se que a biblioteca langchain consiga dividir o documento em chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed41cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "            (\"#\", \"Header_1\"),\n",
    "            (\"##\", \"Header_2\"),\n",
    "            (\"###\", \"Header_3\")\n",
    "        ]\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "chunks = splitter.split_text(result.document.export_to_markdown())\n",
    "\n",
    "chunks[50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2754451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O documento possui 156 chunks de tamanho médio 3225.78 caracteres\n"
     ]
    }
   ],
   "source": [
    "print('O documento possui', len(chunks), 'chunks de tamanho médio', round(sum(len(chunk.page_content) for chunk in chunks) / len(chunks), 2), 'caracteres')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16acee9",
   "metadata": {},
   "source": [
    "#### Transformação dos chunks em vetores (embeddings)\n",
    "\n",
    "Por fim, realiza-se a transformação dos chunks em vetores, e armazena-os para posterior utilização.\n",
    "\n",
    "<i><b>Observação: para este tratamento, é necessário o uso de uma API key da OpenAI. Esta API KEY não estará disponível no repositório, sendo necessário criar um arquivo .env com a seguinte chave:\n",
    "\n",
    "OPENAI_API_KEY: {chave_pessoal_da_openai}</b></i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do modelo de embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuração do modelo de embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Criação da vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"balanco_geral\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"src/vectorStore\", \n",
    ")\n",
    "\n",
    "vector_store.add_documents(chunks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0375ba4",
   "metadata": {},
   "source": [
    "Com isso, a Vector Store (que representa a base de dados) está pronta. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ee550",
   "metadata": {},
   "source": [
    "## Análise Exploratória de Dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34cbbb7",
   "metadata": {},
   "source": [
    "### Entendimento dos dados\n",
    "\n",
    "Conforme mencionado no arquivo \"Projeto Aplicado II.pdf\", os dados a serem utilizados são extraídos de um mesmo documento (\"Balanço Geral do Estado do Rio Grande do Sul 2024\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e314c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('O documento possui', len(chunks), 'chunks de tamanho médio', round(sum(len(chunk.page_content) for chunk in chunks) / len(chunks), 2), 'caracteres')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc6b5f",
   "metadata": {},
   "source": [
    "Foram divididos em 156 chunks, representando possíveis 156 capítulos e/ou seções. \n",
    "\n",
    "Optou-se por criar um dataframe da biblioteca pandas para melhor explorar os dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "estatisticas_chunks = pd.DataFrame(\n",
    "    {\n",
    "        'titulo': [chunk.metadata.values() for chunk in chunks],\n",
    "        'tamanho': [len(chunk.page_content) for chunk in chunks],    # Extração de quantidade de caracteres\n",
    "        'chunk': [chunk.page_content for chunk in chunks]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b343bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estatisticas_chunks.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "estatisticas_chunks.sort_values(by='tamanho', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "estatisticas_chunks.tamanho.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b3f58",
   "metadata": {},
   "source": [
    "Percebe-se acima que há muitos chunks com elevada quantidade de caracteres, em especial o Sumário.\n",
    "Além disso, chama a atenção um chunk com 14 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3809e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estatisticas_chunks.query('tamanho < 1000').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd13da",
   "metadata": {},
   "source": [
    "Como esperado, o chunk de 14 caracteres era uma imagem que acabou não sendo processada. Observando manualmente o Balanço Geral, contudo, percebeu-se ser só uma imagem figurativa, sem valor para o trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca02b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "estatisticas_chunks.tamanho.hist(bins=100)\n",
    "plt.title('Histograma do tamanho dos chunks')\n",
    "plt.xlabel('Tamanho')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5c670",
   "metadata": {},
   "source": [
    "Analisando a frequência das palavras, verifiquemos as mais frequentes nos chunks e em geral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def extrair_palavras_mais_frequentes(texto, n=10):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r'[^a-záéíóúàâêôãõç\\s]', '', texto)\n",
    "    palavras = texto.split()\n",
    "    stopwords = {\"a\", \"o\", \"e\", \"de\", \"do\", \"da\", \"em\", \"um\", \"uma\", \"que\", \"para\", \n",
    "                 \"no\", \"na\", \"os\", \"as\", \"com\", \"por\", \"se\", \"ao\", \"dos\", \"das\"}\n",
    "    palavras_filtradas = [p for p in palavras if p not in stopwords]\n",
    "    contagem = Counter(palavras_filtradas)\n",
    "    \n",
    "    return contagem.most_common(n)\n",
    "\n",
    "\n",
    "for idx, row in estatisticas_chunks.iterrows():\n",
    "    palavras = extrair_palavras_mais_frequentes(row['chunk'])\n",
    "    estatisticas_chunks.at[idx, 'palavras_mais_frequentes'] = palavras\n",
    "\n",
    "textao = ' '.join([chunk for chunk in estatisticas_chunks.chunk])\n",
    "palavras_mais_frequentes = extrair_palavras_mais_frequentes(textao, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc82d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_mais_frequentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0938cbb",
   "metadata": {},
   "source": [
    "## Definição e descrição das bases teóricas dos métodos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a7dcf",
   "metadata": {},
   "source": [
    "### Engenharia de Dados\n",
    "\n",
    "A **Engenharia de Dados** é uma disciplina que se concentra no design, construção e manutenção de sistemas de dados robustos e escaláveis. Ela envolve, entre outros conceitos:\n",
    "\n",
    "- **Pipeline de Dados**: Sequência de processos que extraem, transformam e carregam dados (ETL/ELT)\n",
    "- **Arquitetura de Dados**: Estrutura organizacional que define como os dados fluem através de um sistema\n",
    "- **Data Lake**: Repositório que armazena dados brutos em seu formato nativo\n",
    "\n",
    "\n",
    "#### <i>Relação do trabalho com a disciplina\n",
    "\n",
    "No caso do trabalho em questão, utilizou-se um pipeline básico com as bibliotecas docling e langchain para construir um equivalente a um Data Lake, aqui chamado de Vector Store, com uma arquitetura voltada para a utilização na modelagem e implementação do sistema baseado em LLM.</i>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Inteligência Artificial (IA)\n",
    "\n",
    "A **Inteligência Artificial** é um campo da ciência da computação que visa criar sistemas capazes de realizar tarefas que normalmente requerem inteligência humana:\n",
    "\n",
    "- **Machine Learning (ML)**: Algoritmos que aprendem padrões a partir de dados sem programação explícita\n",
    "- **Deep Learning**: Subcampo do ML que usa redes neurais com múltiplas camadas\n",
    "- **Processamento de Linguagem Natural (NLP)**: Capacidade de máquinas entenderem e processarem linguagem humana\n",
    "- **Computer Vision**: Capacidade de máquinas interpretarem e analisarem informações visuais\n",
    "- **Sistemas de Recomendação**: Algoritmos que sugerem itens relevantes para usuários\n",
    "- **IA Generativa**: Modelos que podem criar conteúdo novo (texto, imagens, código) baseado em padrões aprendidos\n",
    "\n",
    "\n",
    "#### <i> Relação do trabalho com a disciplina\n",
    "\n",
    "No caso deste trabalho, utilizou-se ferramentas de visão computacional e de machine learning, tornadas abstradas na biblioteca docling, de processamento de linguagem natural (para exploração dos dados e embeddings), além de planejar o uso de IA generativa para os próximos passos.</i>\n",
    "\n",
    "\n",
    "### Processamento de Dados Não Estruturados\n",
    "\n",
    "**Dados não estruturados** são informações que não seguem um formato predefinido ou modelo de dados específico:\n",
    "\n",
    "- **Textos**: Documentos, emails, posts em redes sociais, artigos\n",
    "- **Imagens**: Fotos, diagramas, gráficos, documentos digitalizados\n",
    "- **Áudio**: Gravações de voz, música, podcasts\n",
    "- **Vídeo**: Gravações, transmissões ao vivo, conteúdo multimídia\n",
    "- **Dados Semi-estruturados**: JSON, XML, logs de sistema\n",
    "\n",
    "**Técnicas de Processamento**:\n",
    "- **OCR (Optical Character Recognition)**: Conversão de imagens com texto em texto editável\n",
    "- **Web Scraping**: Extração automatizada de dados de websites\n",
    "- **Análise de Sentimento**: Determinação da opinião ou emoção em textos\n",
    "- **Extração de Entidades**: Identificação de pessoas, lugares, organizações em textos\n",
    "- **Embeddings**: Representação vetorial de textos para análise semântica\n",
    "\n",
    "#### <i>Relação do trabalho com a disciplina\n",
    "\n",
    "Conforme já mencionado, a biblioteca Docling utiliza em seu baixo nível instrumentos de OCR e outras técnicas para a transformação dos dados não estruturados. O resultado se torna equivalente a dados semi-estruturados, aptos a receberem uma representação vetorial (embeddings). </i>\n",
    "\n",
    "\n",
    "\n",
    "### Tópicos de Banco de Dados\n",
    "\n",
    "Os **Bancos de Dados** são sistemas organizados para armazenar, gerenciar e recuperar informações:\n",
    "\n",
    "#### **Bancos Relacionais (SQL)**:\n",
    "- **ACID**: Atomicidade, Consistência, Isolamento, Durabilidade\n",
    "- **Normalização**: Processo de organização de dados para reduzir redundância\n",
    "- **Índices**: Estruturas que aceleram consultas\n",
    "- **Transações**: Operações que mantêm integridade dos dados\n",
    "- **Views**: Representações virtuais de dados\n",
    "\n",
    "#### **Bancos NoSQL**:\n",
    "- **Documentos**: MongoDB, CouchDB (armazenam documentos JSON/BSON)\n",
    "- **Chave-Valor**: Redis, DynamoDB (armazenam pares chave-valor)\n",
    "- **Colunas**: Cassandra, HBase (armazenam dados em colunas)\n",
    "- **Grafos**: Neo4j, Amazon Neptune (armazenam relacionamentos)\n",
    "\n",
    "#### **Conceitos Avançados**:\n",
    "- **Data Warehousing**: Repositórios otimizados para análise\n",
    "- **Data Mining**: Descoberta de padrões em grandes volumes de dados\n",
    "- **OLAP vs OLTP**: Processamento analítico vs transacional\n",
    "- **Sharding**: Divisão de dados em múltiplos servidores\n",
    "- **Replicação**: Cópias de dados para alta disponibilidade\n",
    "- **Backup e Recovery**: Estratégias de proteção e recuperação de dados\n",
    "\n",
    "\n",
    "\n",
    "### Resumo da Integração das Disciplinas no Projeto\n",
    "\n",
    "Este projeto integra todas essas áreas ao:\n",
    "1. **Processar dados não estruturados** (PDF) usando técnicas de NLP e OCR\n",
    "2. **Aplicar IA** através de embeddings e modelos de linguagem para análise semântica\n",
    "3. **Utilizar conceitos de banco de dados** na criação de vector stores para busca eficiente\n",
    "4. **Implementar engenharia de dados** no pipeline de transformação e armazenamento dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813493e",
   "metadata": {},
   "source": [
    "## Definição e descrição de como será calculada a acurácia\n",
    "\n",
    "O trabalho possui como desafio <i>\"tornar informações complexas e densas compreensíveis e úteis para o cidadão comum, fortalecendo o exercício do controle social e promovendo uma gestão pública mais aberta e participativa\" </i> (Projeto Aplicado II, p. 8).\n",
    "\n",
    "Isso foi reescrito no objetivo de criar <i>\"um  ChatBOT  interativo,  capaz  de interpretar o Balanço Geral do Estado e demais documentos oficiais, respondendo a consultas  de  usuários sem recorrer  a  terminologia  técnica, de  forma clara e contextualizada.\"</i> (Projeto Aplicado II, p. 8).\n",
    "\n",
    "Uma métrica para o cálculo da acurácia, então, deve levar em conta esse objetivo.\n",
    "\n",
    "\n",
    "A despeito do termo acurácia não ser o melhor conceito a ser aplicado ao projeto, propõe-se que a métrica de avaliação seja a seguir:\n",
    "\n",
    "<i><b>Compreensividade</b></i>: quantidade de respostas que puderam ser compreendidas pelos usuários em relação ao total.\n",
    "\n",
    "\n",
    "Por ser uma métrica de custo elevado de obtenção (dado que não será fácil obter respostas de usuários finais, em especial daqueles que não possuem contato direto com códigos - tecnologia - nem com a matéria contábil), propõe-se a utilização de um <i>proxy</i>:\n",
    "\n",
    "<i><b>Compreensividade Artificial</b</i>: quantidade de respostas que puderam ser compreendidas por LLMs com prompts delimitantes em relação ao toal.\n",
    "\n",
    "\n",
    "Neste caso, <i>prompt delimitante</i> é um prompt que atribui uma personalidade para a LLM e restringe expressamente as respostas e o entendimento baseado no nível de conhecimento esperado daquela personalidade. Por exemplo:\n",
    "\n",
    "\n",
    "Prompt Estruturante de Enfermeiro:\n",
    "\n",
    "\" Você é um ator artístico dedicado. Seu objetivo é representar fielmente um papel de uma pessoa, a fim de que conseguir expressar qual seria a reação dessa pessoa a uma determinada informação.\n",
    "\n",
    "Restrinja seu entendimento sobre a resposta e em relação ao mundo ao que seria esperado de uma pessoa na mesma situação a que te for dada. Não fuja desta ordem.\n",
    "\n",
    "Você receberá um papel e deverá dar feedback a uma determinada frase/texto, retornando uma avaliação de 1 a 5 de compreensividade e a justificativa, conforme os graus a seguir:\n",
    "\n",
    "1. Não compreendeu nada;\n",
    "2. Texto de difícil compreensão\n",
    "3. Texto normal, é capaz de entender \n",
    "4. Texto relativamente adequado ao seu conhecimento\n",
    "5. Texto perfeitamente compreendido por ter usado termos usuais no seu dia a dia\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "Papel: Seu nome é Amanda Sardinha, você é enfermeira no Hospital Central do Rio Grande do Sul. Você nunca teve contato com linguagem de programação nem com qualquer outra matéria que não fosse relacionada a sua vida profissional. Você gosta de correr e ver séries, e sai com os amigos para balada.\n",
    "\n",
    "Texto: \"O crescimento da dívida com a União se deve ao esquema de postergação e refinanciamento  parcial  das  parcelas  possibilitado  pelo  Regime  de  Recuperação  Fiscal  (RRF)  e, sobretudo, à correção monetária do saldo devedor pelo CAM (coeficiente de atualização monetária),  utilizada  para  corrigir  o  saldo  devedor  até  o  mês  de  junho.  A  partir  de  julho, com a entrada em vigor da LC 206/2024, o saldo devedor passou a ser atualizado pelo IPCA. Além disso, a LC 206/2024 suspendeu o pagamento da dívida com a União por 36 meses e  determinou  juro  zerado  nesse  período.  Ao  fim  do  período  previsto,  pela  regra  atual, a atualização do saldo devedor voltará a ser realizada por meio da CAM\"\n",
    "\n",
    "Feedback: {\"compreensividade\": 1, \"justificativa\": \"Eu, como Amanda, não compreendo o que seja Regime de Recuperação Fiscal, nem o que seria LC. Além disso, o texto é confuso para mim.\"}\n",
    "\n",
    "\n",
    "Papel: Seu nome é Amanda Sardinha, você é enfermeira no Hospital Central do Rio Grande do Sul. Você nunca teve contato com linguagem de programação nem com qualquer outra matéria que não fosse relacionada a sua vida profissional. Você gosta de correr e ver séries, e sai com os amigos para balada.\n",
    "\n",
    "Texto: \"A dívida do Estado com o resto do Brasil (União) foi sendo postergada e refinanciada, e aumentou à cada refinanciamento por causa da aplicação de juros e correção monetária por um índice específico (CAM) até junho. Em julho, entrou uma lei que permitiu que a correção monetária fosse feita usando outro índice (IPCA), além de outra lei ter suspendido os pagamentos por 36 meses, com juros zeros enquanto durasse a suspensão. Ao final desse período, os juros vão voltar a ser calculados pelo índice inicial (CAM).\"\n",
    "\n",
    "Feedback: {\"compreensividade\": 3, \"justificativa\": \"Eu, como Amanda, pude compreender que o Estado está em dívida com a União e que houve uma mudança da forma de calcular os juros no período. Mas ainda resta confuso sobre o que isso significa na prática.\"}\n",
    "\n",
    "\"\n",
    "\n",
    "\n",
    "\n",
    "A despeito do proxy não ser perfeito, é uma forma de mensurar quando o sistema estiver razoavelmente pronto para uma anotação mais cara (utilização de usuários finais). A expectativa é que seja montado um dataset de avaliação, contendo diversas perguntas sobre o balanço, e que cada uma das respostas tenha avaliação de 5 a 10 personalidades distintas, obtendo-se a métrica (Compreensividade Artificial) pela média das notas. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b1cad",
   "metadata": {},
   "source": [
    "## Modelagem e Aplicação do Método Analítico\n",
    "\n",
    "Para cumprir com o objetivo, considerando já ter sido feita a análise do entendimento do negócio (ver <i>Projeto Aplicado II.pdf</i>) e a preparação dos dados (ver <i>Tratamento da base de dados</i>), segue-se com a construção de um modelo/arquitetura para consumir os dados e responder às perguntas do usuário final. \n",
    "\n",
    "Uma arquitetura para solucionar este problema:\n",
    "\n",
    "1. Deve receber a mensagem do usuário;\n",
    "2. Deve identificar os dados do usuário de mensagens anteriores. Caso contrário, deverá enviar solicitação dessas informações;\n",
    "3. Deve analisar os dados do usuário (nível de conhecimento técnico)\n",
    "4. Deve adicionar um contexto à mensagem do usuário\n",
    "5. Deve enviar a pergunta para algum modelo (endpoint) \n",
    "6. Deverá retornar a mensagem ao usuário, e aguardar nova iteração\n",
    "\n",
    "\n",
    "Para construir essa solução, pretende-se usar:\n",
    "\n",
    "1. Modelo de Linguagem (LLM): gpt-4o-mini\n",
    "2. Biblioteca <a href='https://www.langchain.com/'>LangChain</a>/<a href='https://langchain-ai.github.io/langgraph/'>LangGraph</a> \n",
    "3. Plataforma <a href=https://huggingface.co/>Hugging.co</a>\n",
    "4. Biblioteca <a href='https://www.gradio.app/'>Gradio</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18fec9",
   "metadata": {},
   "source": [
    "#### Sistema\n",
    "\n",
    "O sistema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
